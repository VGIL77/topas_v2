"""
Phase 8 â€“ SGI Optimizer
Apply SGI-tuned optimization with advanced regularization and scheduling.
Fixed to use trainer_utils helpers consistently.
"""

def run(config, state):
    from trainers.trainer_utils import filter_config, default_sample_fn, safe_model_forward, compute_ce_loss
    from trainers.train_logger import TrainLogger
    from trainers.sgi_optimizer import SGIOptimizer, OptimizerConfig
    import torch

    device = torch.device(config.get("device", "cuda" if torch.cuda.is_available() else "cpu"))
    
    # Get model from state
    model = state.get("model")
    if model is None:
        print("[Phase 8] Model not found in state. Run Phase 0 first.")
        return state
    
    logger = state.get("logger") or TrainLogger(config.get("log_dir", "logs"))
    
    # Init SGI optimizer with filtered config
    try:
        sgi_config_dict = filter_config(config, OptimizerConfig.__annotations__)
        sgi_config = OptimizerConfig(**sgi_config_dict)
        sgi_opt = SGIOptimizer(model, sgi_config)
    except Exception as e:
        print(f"[Phase 8] Error creating SGI optimizer: {e}. Using defaults.")
        sgi_config = OptimizerConfig()
        sgi_opt = SGIOptimizer(model, sgi_config)
    
    state["sgi_optimizer"] = sgi_opt
    
    # Dataset
    dataset = state.get("dataset")
    
    epochs = config.get("epochs", 2)
    steps_per_epoch = config.get("steps_per_epoch", 200)
    global_step = state.get("global_step", 0)
    
    print(f"[Phase 8] Starting SGI Optimizer for {epochs} epochs")
    
    for epoch in range(epochs):
        epoch_loss = 0.0
        num_steps = 0
        
        for step in range(steps_per_epoch):
            try:
                # Use default_sample_fn instead of state["sample_fn"]
                demos, test = default_sample_fn(dataset, device)
                
                # Fix CE loss issue: clamp targets to valid range [0, 9] and ensure integer type
                if test.get("output") is not None:
                    test["output"] = test["output"].long().clamp(0, 9)
                
                # Forward pass using safe wrapper
                grid, logits, extras = safe_model_forward(model, demos, test, device, training_mode=True)
                
                # Get target
                target = test.get("output")
                if target is None:
                    raise RuntimeError("[Phase 8] No test output found in ARC data. Cannot train SGI optimizer without valid targets!")
                
                # Compute loss using compute_ce_loss helper
                loss = compute_ce_loss(logits, target)
                
                # SGI optimization step
                try:
                    metrics = sgi_opt.step(loss)
                except Exception as e:
                    print(f"[Phase 8] SGI step error: {e}. Using regular backward.")
                    # Fallback to regular optimization
                    loss.backward()
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                    for param_group in sgi_opt.optimizer.param_groups:
                        for param in param_group['params']:
                            if param.grad is not None:
                                param.data -= param_group['lr'] * param.grad
                    sgi_opt.optimizer.zero_grad()
                    metrics = {"lr": config.get("learning_rate", 1e-4), "grad_norm_before": 0.0}
                
                epoch_loss += loss.item()
                num_steps += 1
                global_step += 1
                
                # Logging
                if step % config.get("log_interval", 20) == 0:
                    logger.log_batch(global_step, {
                        "phase": "8_sgi_optimizer",
                        "epoch": epoch,
                        "step": step,
                        "total": loss.item(),
                        "lr": metrics.get("lr", 0.0),
                        "grad_norm": metrics.get("grad_norm_before", 0.0)
                    })
                    
            except Exception as e:
                print(f"[Phase 8] Error in step {step}: {e}")
                continue
        
        # Epoch-level step with real accuracy
        if num_steps > 0:
            avg_loss = epoch_loss / num_steps
            accuracy = max(0.0, 1.0 - avg_loss / 10.0)  # Simple accuracy estimate
            
            try:
                sgi_opt.epoch_step(epoch, {"accuracy": accuracy})
            except Exception as e:
                print(f"[Phase 8] Epoch step error: {e}")
            
            logger.log_epoch(epoch, "8_sgi_optimizer", avg_loss, {"accuracy": accuracy})
            print(f"[Phase 8] Epoch {epoch} - Avg Loss: {avg_loss:.4f}, Est Accuracy: {accuracy:.2%}")
    
    state.update({
        "sgi_optimizer": sgi_opt,
        "logger": logger,
        "global_step": global_step,
        "phase8_complete": True
    })
    
    print("[Phase 8] SGI Optimizer complete")
    return state